---
title: Airport traffic
subtitle: Applied
date: "2023-12-05"
author: 
- name: "Adrien Berard"
- name: "Nathan Pizzetta"
- name: "Louis Rodriguez"
- name: "Sigurd Saue"

format:
  html:
    toc: true
    toc-depth: 3
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(dplyr)
library(zoo)
library(xts)
library(tidyr)
library(magrittr)
library(ggplot2)
library(tseries)
library(forecast)
```


```{r, echo=FALSE,  warning=FALSE, message=FALSE}
# Format of our dataframes
styled_dt <- function(df, n=5) {

  DT::datatable(df, 
  extensions = 'Buttons',
  rownames = FALSE,
  class = 'dataTables_wrapper',
  options = list(
    scrollX = TRUE, 
    pageLength = n,
    dom = 'Bfrtip',
    buttons = c('copy', 'csv', 'excel')
  ))
  }
```

# Data import

:::{.panel-tabset}

### Traffic data
This dataset includes the monthly number of passengers from 1998 to 2023 in different european airports.

```{r, warning=FALSE, message=FALSE}
# Global data
traffic <- openxlsx::read.xlsx(xlsxFile="datasets/data_airports_APP.xlsx")

# Simplified label
traffic <- traffic %>% dplyr::rename("Airport" = "REP_AIRP.(Labels)")
```

### Localisation data
This dataset associates the airport with its country.

```{r}
# Country
airports_names<- read.csv("datasets/airports_by_country.csv")

airports_names$Airport <- paste(airports_names$Airport, "airport", sep = " ")
airports_names <- airports_names %>% mutate(Country = ifelse(Country == "Chile", "Spain", Country))
```

### Members of European Union data
This dataset associates 1 to European Union members and 0 to the rest.

```{r}
# EU
eu_countries_2019 <- read.csv("datasets/eu_countries_2019.csv")
```

:::

&nbsp;

# Pre-processing

We choose to keep the data from 2002 to 2023. Before 2022 we have only few data available and it seems not interesting for our study.

```{r}
# Selection
names_col <- names(traffic)
selected_col <- c(names_col[1], names_col[50:length(names_col)])
traffic <- traffic %>% dplyr::select(all_of(selected_col))
```

&nbsp;

Here we aggregate the country associated to each airport. We will need them for our analysis and to create our segmentation by country afterward.

```{r}
# Merged
traffic_mg <- merge(traffic, airports_names, by = "Airport", all.x = TRUE)
```

&nbsp;

We first check if in our data we have some duplicated lines.
```{r}
airports_dupli <- duplicated(traffic_mg)
length(traffic_mg[airports_dupli,])
```

And then apply `unique`.
```{r}
# Duplicates erase
traffic_mg <- unique(traffic_mg)
```

&nbsp;

We check if some of the airports are not associated with a country in our dataset `airports_names`.

```{r}
# Checking
airports_without_country <- traffic_mg[is.na(traffic_mg$Country), ]
```

```{r, echo=FALSE}
styled_dt(airports_without_country, 5)
```

&nbsp;

Our dataset gives a report of the number of passengers carried by the airports each month starting in January
of 1998 to september of 2023.

```{r, echo=FALSE, warning=FALSE}
styled_dt(traffic_mg, 5)
```

&nbsp;

We there modify our dataset structure to prevent issues with `pivot_longer`.

```{r}
# Modification
traffic_pivot <- tidyr::pivot_longer(traffic_mg, cols = -c("Airport", "Country"), names_to = "Date", values_to = "Passengers")

# Managing Nan
traffic_pivot$Passengers[traffic_pivot$Passengers == ":"] <- 0

# Numerical values
traffic_pivot$Passengers <- as.numeric(traffic_pivot$Passengers)

# Date
traffic_pivot$Date <- zoo::as.Date(paste0(traffic_pivot$Date, "-01"), format="%Y-%m-%d")
```

## Selection of the most relevant european airports

For this, our goal is to keep at least one airport by country. To do so, we will focus on the airports with the most attendace in every country.

&nbsp;

First, we sum the total number of passengers between 2002 and 2023 :
```{r}
# Sum
traffic_sum <- traffic_pivot %>% group_by(Airport, Country) %>% summarise(sumPassengers = sum(Passengers))
```

Then, we select the most relevant airport of every country :
```{r}
# Selection
airports_best_ranked <- traffic_sum %>% group_by(Country) %>% slice_max(order_by = sumPassengers)
```

For 3 countries we have no data. Therefore, we delete them. At the same time, we also erase some territories of no interest and issues. For that we previously checked that in our list we do not have any big airport that could be pertinent.

```{r}
# Erase
airports_best_ranked <- airports_best_ranked %>% filter(sumPassengers != 0)

list_countries = c("Faroe Islands (Denmark)", "Fictional/Private", "French Guiana", 
  "Guadeloupe (France)", "Martinique (France)", "Mayotte (France)", "Reunion (France)",
  "Saint Barthelemy (France)", "Saint Martin (France)", "Svalbard (Norway)", NA)

airports_best_ranked <- airports_best_ranked %>% filter(!(Country %in% list_countries))
```


# Final dataset

Here is our dataset that we will use from now on to build our model and make our analysis.

```{r}
# Final dataset
airports_final_list <- unique(airports_best_ranked$Airport)

traffic_checked <- traffic_pivot %>% filter(Airport %in% airports_final_list)
```


```{r, echo=FALSE}
write.csv(traffic_checked, file = "datasets/data_airports_cleaned_APP.csv", row.names = FALSE, na = "NA")
styled_dt(traffic_checked)
```

# Plot of our data

## Overview

Here we visualize how is the general tendance with all our airports.

### EU members

In our data, we do not have only countries taking part of the European Union but also other european countries that are not member of it.
We thus plot there distribution to visualize it.

```{r}
# EU
eu_sum <- eu_countries_2019 %>% group_by(EU_2019) %>% count()
eu_sum <- eu_sum %>% mutate(EU_2019 = ifelse(EU_2019 == 1, "Member", "Non member"))

# Plot
ggplot(eu_sum, aes(x = EU_2019, y = n, fill = EU_2019)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = n), vjust = 3, colour = 'white', size = 4) +
  labs(title = "Repartition of european countries", x = "", y = "Number of countries", fill = "European Union (2019)") +
  theme_minimal()
```
### Ranking on the total airport traffic

We ranked our countries by their total passenger traffic. This will help us to make an analysis on the most relevant airport of our dataset.

```{r}
ggplot2::ggplot(airports_best_ranked, aes(x = sumPassengers, y = reorder(Country, sumPassengers))) +
  geom_bar(stat = "identity", fill = "green") +
  labs(title = "Ranking of Countries based on their traffic",
       x = "Total passengers carried",
       y = "Country")
```


### Global trend on our data

:::{.panel-tabset}

### All airports 1

```{r, echo=FALSE}
ggplot2::ggplot(traffic_checked, aes(x = Date, y = Passengers, color = Country)) +
  geom_line() +
  theme_minimal() +
  labs(title = "Monthly Passengers per Country", x = "Date", y = "Number of Passengers")
```

### All airports 2

We transform our dataset as a time serie
```{r}
traffic_checked_xts <- traffic_checked$Passengers %>% xts::xts(order.by=traffic_checked$Date)
```

```{r, echo=FALSE}
autoplot(traffic_checked_xts) +
  xlab("Date") +
  ylab("Number of passengers") +
  ggtitle("Monthly passengers per country ")
```

### General
```{r}
traffic_mean <- traffic_checked %>%
  group_by(Date) %>%
  summarize(MeanPassengers = mean(Passengers, na.rm = TRUE))

dygraphs::dygraph(traffic_mean, main = "Average Passengers per Month", xlab = "Date")
```

:::

&nbsp;

## Focus

Quick view of the 3 airports with most traffic in our dataset.

:::{.panel-tabset}

### CDG

#### Paris Charles de Gaulles Airport

```{r}
charles <- traffic_checked %>% dplyr::filter(Airport == "PARIS-CHARLES DE GAULLE airport")
```

Formating the dataset as a time serie variable

```{r}
# Time serie function
charles_ts <- charles %>% dplyr::select(4) %>% ts(frequency = 12, start = c(2002, 1))
```

```{r, echo=FALSE}
dygraphs::dygraph(data=charles_ts, main="Passengers per month at Paris Charles de Gaulle")
```

### LHR

#### London Heathrow Airport

```{r}
heathrow <- traffic_checked %>% dplyr::filter(Airport == "LONDON HEATHROW airport")
```

Formating the dataset as a time serie variable

```{r}
# Time serie function
heathrow_ts <- heathrow %>% dplyr::select(4) %>% ts(frequency = 12, start = c(2002, 1))
```

```{r, echo=FALSE}
dygraphs::dygraph(data=heathrow_ts, main="Passengers per month at London Heathrow")
```

### AMS

#### Amsterdam Schiphol airport

```{r}
schiphol <- traffic_checked %>% dplyr::filter(Airport == "AMSTERDAM/SCHIPHOL airport")
```

Formating the dataset as a time serie variable

```{r}
# Time serie function
schiphol_ts <- schiphol %>% dplyr::select(4) %>% ts(frequency = 12, start = c(2002, 1))
```

```{r, echo=FALSE}
dygraphs::dygraph(data=schiphol_ts, main="Passengers per month at Amsterdam-Schiphol")
```

:::

# Our Time Serie model

First of all, we want to keep the data only until the end of 2019, period before COVID impact if we refer to our plots.

```{r}
charles_ts19 <- charles %>% dplyr::select(4) %>% ts(frequency = 12, start = c(2002, 1), end = c(2019, 12))
```


## Study on our data

### Stationarity and parameters

As first though, we saw previously on the graph of the Paris Charles de Gaulle airport that there is an ascending trend. This violates the assumption of same mean at all time in the stationarity properties. Also, distinct seasonal patterns are present which is also a violation of the previous requirement.
We thus already think that our time series are not stationary.

&nbsp;

The positive trend seems to be linear which suggests that a first difference could be sufficient to detrend it. In the case of curve we would have preferred to previously transform our data and then make a first difference.
But before doing a first difference we have to take the seasonality into account. If after the seasonal differencing the trend remains, then we will apply a forst difference.

&nbsp;

We will verify it as it follows.

&nbsp;

First, we study the non-seasonal behavior. It is likely that the short run non-seasonal components will contribute to the model.
We thus take a look at the ACF and PACF behavior under the seasonality lag (12) to assess what non-seasonal components might be.

### No differencing

We start with anakysing our time serieswithout previous differencing.

```{r}
# Trend
ts_decomposed <- decompose(charles_ts19)

# Plot
plot(charles_ts19, col = 'blue', xlab = "Year", ylab = "Passengers",
     main = "CDG passengers before seasonal differencing")

lines(ts_decomposed$trend,  col = 'red')

legend("topright", legend = c("Original", "Trend"), col = c("blue", "red"), 
       lty = c(1, 1), cex = 0.8)
```

On both ACF and PACF graphs, the blue dashed lines represent values beyond which the ACF and PACF are significantly different from zero at $5\%$ level. These lines represent the bounds of the $95\%$ confidence intervals.
A bar above are under these lines would suppose a correlation. On the contrary, a bar between these two lines would suppose zero correlation.

:::{.panel-tabset}

#### ADF
We test whether our time series is stationary or not with the Augmented Dickey-Fuller Test.

$H_0 : Unit root$
$H_1 : Stationary$

```{r}
charles_ts19 <- na.omit(charles_ts19)
adf_test <- adf.test(charles_ts19, alternative = "stationary")

# Results of the ADF test
print(adf_test)
```
Our result with a $p-value = 0.01 < 0.05$ let us suppose that there is stationarity. We success to reject the null hypothesis that states a unit root (non-stationarity) in the time serie.

This let us suppose that we could need to differenciate `charles_ts19`.

#### ACF

ACF for Autocorrelation Function


This graph is useful to identify the number of MA(q) (Moving Average) terms in our model.

We will interpret it as the following :

- If the ACF shows a slow exponential or sinusoidal decay, it could suggests an AR process.
- If the ACF cuts off after a specific delay (lag), it could suggests an AM process.

```{r, echo=FALSE}
# Plot of the Autocorrelation Function (ACF)
forecast::ggAcf(charles_ts19) +
  ggplot2::ggtitle("Sample ACF for CDG airport")
```
The ACF shows a gradual decline but with several significant spikes at various lags.
There are significant lags at intervals that could suggest a seasonal pattern, which is in line with our year seasonality. Our data frequency is monthly, and these intervals correspond to the number 12.

The fact that there are significant autocorrelations at multiple lags might also suggest that our data is not stationary, and differencing (either seasonal or non-seasonal) might be required to achieve stationarity.

#### PACF

PACF for Partial Autocorrelation Function


This graph is useful to identify the number of AR(p) (AutoRegressive) terms in our model.

We will interpret it as the following :

- If the PACF shows a slow exponential or sinusoidal decay, it could indicates an MA process.
- If the PACF cuts off after a certain delay, it could indicates an AR process.

```{r, echo=FALSE}
# Plot of the Partial Autocorrelation Function (PACF)
forecast::ggPacf(charles_ts19) +
  ggplot2::ggtitle("Sample PACF for CDG airport")
```
The PACF shows a significant spike at lag 1 and then cuts off, which indicates that a non-seasonal AR(1) component may be present in our time series.
The other lags do not appear to be significantly different from zero, suggesting that no higher-order AR terms are needed.

:::

### Seasonal differencing

We will take the seasonality into account, which means that we make a difference at lag 12 because of our monthly data.

```{r}
# Difference
charles_ts19_diff <- diff(charles_ts19, lag = 12)

# Trend
ts_stl <- stl(charles_ts19_diff, s.window = "periodic")

# Plot
plot(charles_ts19_diff, col = 'blue', xlab = "Year", ylab = "Passengers",
     main = "CDG passengers after seasonal differencing")

lines(ts_stl$time.series[, "trend"], col = 'red')

legend("topright", legend = c("Original", "Trend"), col = c("blue", "red"), 
       lty = c(1, 1), cex = 0.8)
```

After differencing, we check again for stationarity. Here, taking a look at the graph, it seems that there is no remaining trend.
To validate our proposal, we run the ADF test with the ACF and PACF graphs and look for stationarity.

:::{.panel-tabset}

#### ADF

$H_0 : Unit root$
$H_1 : Stationary$

```{r}
adf_test2 <- adf.test(charles_ts19_diff, alternative = "stationary")

# Results of the ADF test
print(adf_test2)
```
Again, we reject the null hypothesis at a $5\%$ significance level.
Which means that we still have stationarity in our time series.

#### ACF

```{r, echo=FALSE}
# Plot of the Autocorrelation Function (ACF)
forecast::ggAcf(charles_ts19_diff) +
  ggplot2::ggtitle("Sample ACF for CDG airport after differencing")
```



#### PACF

```{r, echo=FALSE}
# Plot of the Partial Autocorrelation Function (PACF)
forecast::ggPacf(charles_ts19_diff) +
  ggplot2::ggtitle("Sample PACF for CDG airport after differencing")
```

:::

## Modeling

### Automated parameters choice

:::{.panel-tabset}

#### Model

R has an automated function that can help us to build our model. We thus run it to get the suggested coefficients with : `auto.arima`.

```{r}
# Get suggestion
forecast::auto.arima(charles_ts19)
```

Based on the results we create the following model.

```{r}
# SARIMA
sarima_model <- Arima(charles_ts19, order=c(1,0,0), seasonal=list(order=c(0,1,1), period=12))
```

Now that we have build our model, we check the residuals.

#### Residuals

Interpretation of the residuals :

```{r}
checkresiduals(sarima_model)
```

- **The Ljung-Box test**, we got $p-value = 1.416*10^{-08} < 0.05$, we reject the null hypothesis. This suggests that the error has a significant autocorrelation at some of the lags used in the test (up to lag 24 here). Our model may not capture adequately  the time-dependency.

- **The residuals time series plot**, helps us to check if our residuals look like white noise (meaning that our model has captured the underlying process correctly). Here, our residuals fluctuate around zero, we do not see obvious patterns which is a good sign.

- **The ACF of residuals**, helps us to check the autocorrelation within our residuals. In a good model we would expect the bars to be between both dashed blue lines. In our case, we see some of the bars outside the confidence intervals. Thus, there may be some autocorrelation in the residuals that our model does not capture.

- **The histogram and density plot**, shows the distribution of the residuals along with the density curve of the normal distribution for comparison. By assumption, our residuals should look like a normal distribution with a mean of zero. Here, we have large pikes around zero, but also some big deviations around. Our residuals may not be normally distributed.


#### Prediction

```{r}
# Prediction of the 12 next months
forecasts <- forecast(sarima_model, h=12)
#str(forecasts)
plot(forecasts)
```

#### Summary

```{r}
# Information on the model
sarima_model
```

:::

### Manual parameters choice

:::{.panel-tabset}

#### AIC

The Aikaike Information Criterion : the goal is to minimize this criterion with the different parameters of our model.
The parameters obtained are the following :`r min_AIC_params_charles`

#### BIC

The Bayesian Information Criterion : the goal is to minimize this criterion with the different parameters of our model.
The parameters obtained are the following : `r min_BIC_params_charles`



:::


## MADRID

::: panel-tabset
#### AIC

The parameters obtained are the following :`r min_AIC_params_madrid`

#### BIC

The parameters obtained are the following :`r min_BIC_params_madrid`

:::

## ROMA

::: panel-tabset
#### AIC

The parameters obtained are the following :`r min_AIC_params_roma`

#### BIC

The parameters obtained are the following :`r min_BIC_params_roma`

:::

## KOBENHAVN

::: panel-tabset
#### AIC

The parameters obtained are the following :`r min_AIC_params_kob`

#### BIC

The parameters obtained are the following :`r min_BIC_params_kob`

:::

## OSLO

::: panel-tabset
#### AIC

The parameters obtained are the following :`r min_AIC_params_oslo`

#### BIC

The parameters obtained are the following :`r min_BIC_params_oslo`

:::

# SARIMA

## Model for PARIS

>>>>>>> Stashed changes
#### Parameters with AIC

:::{.panel-tabset}

#### Model
```{r}
sarima_model2 <- Arima(charles_ts19, order=c(2,1,1), seasonal=list(order=c(0,1,1), period=12))
```

#### Residuals

Interpretation of the residuals :

```{r}
checkresiduals(sarima_model2)
```

#### Prediction

```{r}
# Prediction of the 12 next months
forecasts2 <- forecast(sarima_model2, h=12)
#str(forecasts)
plot(forecasts2)
```
#### Summary

```{r}
# Information on the model
sarima_model2
```


:::

#### Parameters with BIC

:::{.panel-tabset}

#### Model
```{r}
sarima_model3 <- Arima(charles_ts19, order=c(0,1,1), seasonal=list(order=c(0,1,1), period=12))
```

#### Residuals

Interpretation of the residuals :

```{r}
checkresiduals(sarima_model3)
```

#### Prediction

```{r}
# Prediction of the 12 next months
forecasts3 <- forecast(sarima_model3, h=12)
#str(forecasts)
plot(forecasts3)
```

#### Summary

```{r}
# Information on the model
sarima_model3
```


:::


### Politics

We add 3 politics to the dataset :
 - Borders main UE Period
 - Border non-UE Period
 - Negative tests Period

We add them as dummies over the same period of our time series. We put a 1 when the policy is applied, and 0 when it is not. We did it on Excel, it was simplier than with R.


```{r}
# Global dataset
airports_politics <- openxlsx::read.xlsx(xlsxFile="datasets/DATA_POLITICS.xlsx")

# Dataset for politics and airports
col_start <- which(names(airports_politics) == "2002-01")
col_end <- which(names(airports_politics) == "2023-09")
library(dplyr)
airports <- airports_politics %>% dplyr::select(1, col_start : col_end)
politics <- airports_politics %>% dplyr::select(1, (col_end+1):length(airports_politics))

# Dataset for each policy

#a
col_start_a <- which(names(airports_politics) == "2002-01.a")
col_end_a <- which(names(airports_politics) == "2023-09.a")
policy_a <- airports_politics %>% dplyr::select(1, col_start_a : col_end_a)

#b
col_start_b <- which(names(airports_politics) == "2002-01.b")
col_end_b <- which(names(airports_politics) == "2023-09.b")
policy_b <- airports_politics %>% dplyr::select(1, col_start_b : col_end_b)

#c
col_start_c <- which(names(airports_politics) == "2002-01.c")
col_end_c <- which(names(airports_politics) == "2023-09.c")
policy_c <- airports_politics %>% dplyr::select(1, col_start_c : col_end_c)
```

```{r}
# Pivot
policy_a <- tidyr::pivot_longer(policy_a, cols = -c("Airport"), names_to = "Date", values_to = "Borders main EU period")
policy_b <- tidyr::pivot_longer(policy_b, cols = -c("Airport"), names_to = "Date", values_to = "Borders non-EU period")
policy_c <- tidyr::pivot_longer(policy_c, cols = -c("Airport"), names_to = "Date", values_to = "Negative tests period")
airports <- tidyr::pivot_longer(airports, cols = -c("Airport"), names_to = "Date", values_to = "Passenger")

# Formating date
policy_a$Date <- gsub("\\.a$", "", policy_a$Date)
policy_b$Date <- gsub("\\.b$", "", policy_b$Date)
policy_c$Date <- gsub("\\.c$", "", policy_c$Date)
```

```{r}
politics_formate <- merge(airports, policy_a, by = c("Airport", "Date"), all.x = TRUE)
politics_formate <- merge(politics_formate, policy_b, by = c("Airport", "Date"), all.x = TRUE)
politics_formate <- merge(politics_formate, policy_c, by = c("Airport", "Date"), all.x = TRUE)
```



